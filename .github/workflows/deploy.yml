name: Build and Deploy

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

permissions:
  contents: read
  packages: write
  security-events: write

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository_owner }}/codeforces-multi-cloud-microservices

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [auth-service, contest-service, submission-service, execution-service, scoring-service, leaderboard-service]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Convert repository owner to lowercase
      id: repo
      run: echo "owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.repository_owner }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      id: docker
      with:
        context: ./services/${{ matrix.service }}
        push: ${{ github.event_name != 'pull_request' }}
        tags: |
          ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/codeforces-${{ matrix.service }}:${{ github.sha }}
          ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/codeforces-${{ matrix.service }}:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64
    
    - name: Run security scan
      uses: aquasecurity/trivy-action@master
      if: github.event_name != 'pull_request' && steps.docker.outcome == 'success'
      id: trivy-scan
      continue-on-error: true
      with:
        image-ref: ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/codeforces-${{ matrix.service }}:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-results-${{ matrix.service }}.sarif'
        exit-code: '0'
    
    - name: Check if Trivy results exist
      id: check-trivy
      if: github.event_name != 'pull_request' && steps.trivy-scan.outcome != 'failure'
      run: |
        if [ -f "trivy-results-${{ matrix.service }}.sarif" ]; then
          echo "exists=true" >> $GITHUB_OUTPUT
        else
          echo "exists=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Upload Trivy results to GitHub Security
      uses: github/codeql-action/upload-sarif@v4
      if: github.event_name != 'pull_request' && steps.check-trivy.outputs.exists == 'true'
      with:
        sarif_file: 'trivy-results-${{ matrix.service }}.sarif'
        wait-for-processing: false

  build-frontend:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Convert repository owner to lowercase
      id: repo
      run: echo "owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.repository_owner }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push frontend Docker image
      uses: docker/build-push-action@v4
      with:
        context: ./frontend/nextjs-app
        push: ${{ github.event_name != 'pull_request' }}
        tags: |
          ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/codeforces-frontend:${{ github.sha }}
          ${{ env.REGISTRY }}/${{ steps.repo.outputs.owner }}/codeforces-frontend:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64

  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: codeforces
          POSTGRES_PASSWORD: codeforces_test
          POSTGRES_DB: codeforces_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd services/auth-service
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Initialize test database
      env:
        DATABASE_URL: postgresql://codeforces:codeforces_test@localhost:5432/codeforces_test_db
        PYTHONPATH: ${{ github.workspace }}/services/auth-service
      run: |
        cd services/auth-service
        python -c "from app.database import init_db; init_db()"
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://codeforces:codeforces_test@localhost:5432/codeforces_test_db
        PYTHONPATH: ${{ github.workspace }}/services/auth-service
      run: |
        cd services/auth-service
        pytest tests/ -v --cov=app --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./services/auth-service/coverage.xml

  deploy:
    needs: [build, build-frontend, test]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.5.0
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1
    
    - name: Configure Azure credentials
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
      continue-on-error: true
    
    - name: Terraform Init
      run: |
        cd infrastructure/terraform/aws && terraform init
        cd ../azure && terraform init
    
    - name: Terraform Plan
      continue-on-error: true
      run: |
        cd infrastructure/terraform/aws && \
        # Check if EKS cluster exists
        echo "Checking if EKS cluster exists..."
        if aws eks describe-cluster --name codeforces-aws-cluster --region us-east-1 2>&1 | grep -q "ResourceNotFoundException"; then
          echo "✗ EKS cluster does not exist, will create new cluster and VPC"
          EKS_EXISTS=false
          VPC_EXISTS=false
        else
          echo "✓ EKS cluster exists, will use existing cluster and VPC"
          EKS_EXISTS=true
          VPC_EXISTS=true
        fi && \
        terraform plan -out=tfplan \
          -var="import_existing_iam_role=true" \
          -var="import_existing_eks_cluster=$EKS_EXISTS" \
          -var="import_existing_vpc=$VPC_EXISTS" || echo "AWS plan failed"
        cd ../azure && \
        # Check if AKS cluster exists
        echo "Checking if AKS cluster exists..."
        if az aks show --resource-group codeforces-rg --name codeforces-aks-cluster --output json >/dev/null 2>&1; then
          echo "✓ AKS cluster exists, will use existing"
          AKS_EXISTS=true
        else
          echo "✗ AKS cluster does not exist, will create new"
          AKS_EXISTS=false
        fi
        
        echo "Setting import_existing_aks_cluster=$AKS_EXISTS"
        echo "Note: PostgreSQL server will use default (import_existing_postgres=true from variables.tf)"
        terraform plan -out=tfplan \
          -var="import_existing_resource_group=true" \
          -var="import_existing_vnet=true" \
          -var="import_existing_public_ip=true" \
          -var="import_existing_subnet_main=true" \
          -var="import_existing_subnet_postgres=true" \
          -var="import_existing_aks_cluster=$AKS_EXISTS" \
          -var="import_existing_dns_zone=true" \
          -var="import_existing_dns_vnet_link=true" \
          -var="import_existing_lb=true" \
          -var="db_admin_login=${{ secrets.DB_ADMIN_LOGIN }}" \
          -var="db_admin_password=${{ secrets.DB_ADMIN_PASSWORD }}" && \
        echo "✓ Azure plan created successfully" || \
        (echo "✗ Azure plan failed" && ls -la tfplan 2>&1 || echo "Plan file not found")
    
    - name: Terraform Apply
      if: github.ref == 'refs/heads/main'
      continue-on-error: true
      run: |
        cd infrastructure/terraform/aws && \
        if [ -f tfplan ]; then
          terraform apply -auto-approve tfplan || echo "AWS apply failed"
        else
          echo "⚠ AWS plan file not found, skipping apply"
        fi
        cd ../azure && \
        if [ -f tfplan ]; then
          terraform apply -auto-approve tfplan || echo "Azure apply failed"
        else
          echo "⚠ Azure plan file not found, skipping apply"
        fi
    
    - name: Install kubectl
      uses: azure/setup-kubectl@v3
      continue-on-error: true
    
    - name: Deploy to AWS EKS
      continue-on-error: true
      run: |
        aws eks update-kubeconfig --name codeforces-aws-cluster --region us-east-1 || echo "AWS EKS config failed"
        kubectl apply --validate=false -f infrastructure/kubernetes/base/namespace.yaml || echo "Namespace creation failed"
        find infrastructure/kubernetes/base/ -name "*.yaml" ! -name "kustomization.yaml" -exec kubectl apply --validate=false -f {} \; || echo "AWS deployment failed"
        kubectl apply --validate=false -f infrastructure/kubernetes/network-policies.yaml || echo "Network policies failed"
        kubectl apply --validate=false -f infrastructure/kubernetes/rbac.yaml || echo "RBAC failed"
    
    - name: Deploy to Azure AKS
      continue-on-error: true
      env:
        DB_PASSWORD: ${{ secrets.DB_ADMIN_PASSWORD }}
        JWT_SECRET: ${{ secrets.JWT_SECRET_KEY }}
      run: |
        az aks get-credentials --resource-group codeforces-rg --name codeforces-aks-cluster || echo "Azure AKS config failed"
        kubectl apply --validate=false -f infrastructure/kubernetes/base/namespace.yaml || echo "Namespace creation failed"
        
        # Create secrets
        kubectl create secret generic db-secret \
          --from-literal=url="postgresql://${{ secrets.DB_ADMIN_LOGIN }}:${DB_PASSWORD}@codeforces-postgres.postgres.database.azure.com:5432/codeforces_db?sslmode=require" \
          --namespace=codeforces \
          --dry-run=client -o yaml | kubectl apply -f - || echo "DB secret creation failed"
        
        kubectl create secret generic redis-secret \
          --from-literal=url="redis://redis:6379/0" \
          --namespace=codeforces \
          --dry-run=client -o yaml | kubectl apply -f - || echo "Redis secret creation failed"
        
        kubectl create secret generic jwt-secret \
          --from-literal=secret-key="${JWT_SECRET:-your-secret-jwt-key-change-in-production}" \
          --namespace=codeforces \
          --dry-run=client -o yaml | kubectl apply -f - || echo "JWT secret creation failed"
        
        kubectl create secret generic rabbitmq-secret \
          --from-literal=url="amqp://guest:guest@rabbitmq:5672/" \
          --namespace=codeforces \
          --dry-run=client -o yaml | kubectl apply -f - || echo "RabbitMQ secret creation failed"
        
        find infrastructure/kubernetes/base/ -name "*.yaml" ! -name "kustomization.yaml" -exec kubectl apply --validate=false -f {} \; || echo "Azure deployment failed"
        kubectl apply --validate=false -f infrastructure/kubernetes/network-policies.yaml || echo "Network policies failed"
        kubectl apply --validate=false -f infrastructure/kubernetes/rbac.yaml || echo "RBAC failed"
    

